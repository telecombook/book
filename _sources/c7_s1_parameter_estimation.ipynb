{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation in White Gaussian Noise\n",
    "\n",
    "Assume that $ k $ samples of the measured signal $ y_i $, taken over a period $ T $, are real with\n",
    "\n",
    "$$\n",
    "y_i = s_i(\\alpha) + n_i, \\quad i = 1, \\ldots, k\n",
    "$$\n",
    "\n",
    "where \n",
    "* $ \\alpha $ is the parameter to be estimated\n",
    "* $ s_i(\\alpha) $, $ i = 1, \\ldots, k $, are samples of the signal \n",
    "* $ n_i $, $ i = 1, \\ldots, k $, are samples of zero-mean, white Gaussian noise with variance $ \\sigma^2 $. \n",
    "\n",
    "Note that $\\sigma^2 = N_0 B$ for passband.\n",
    "\n",
    "Let $ \\vec{y} $ be the set of samples $ y_i $, $ i = 1, \\ldots, k $.\n",
    "\n",
    "The pdf $ p(\\vec{y}|\\alpha) $ can be expressed as\n",
    "\n",
    "$$\n",
    "p(\\vec{y}|\\alpha) = \\frac{1}{(2\\pi\\sigma^2)^{k/2}} \\exp \\left[ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^k (y_i - s_i(\\alpha))^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP Estimation\n",
    "\n",
    "A MAP estimate of $ \\alpha $ can then be obtained by finding the maximum of $ \\ln p(\\alpha|\\vec{y}) $ or equivalently, by finding the maximum of $ \\ln[p(\\vec{y}|\\alpha) p(\\alpha)] $, i.e.,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\alpha} \\ln p(\\vec{y}|\\alpha) + \\frac{\\partial}{\\partial \\alpha} \\ln p(\\alpha) = 0\n",
    "$$\n",
    "\n",
    "Pluggin $p(\\vec{y}|\\alpha)$, a MAP estimate to be obtained as the solution to\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2} \\sum_{i=1}^k (y_i - s_i(\\alpha)) \\frac{\\partial s_i(\\alpha)}{\\partial \\alpha} + \\frac{\\partial}{\\partial \\alpha} \\ln p(\\alpha) = 0\n",
    "$$\n",
    "\n",
    "To proceed further, the form of the signal and the a priori pdf $ p(\\alpha) $ (if $ \\alpha $ is random) must be known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Estimation\n",
    "\n",
    "An ML estimate can be obtained by finding the maximum of $ p(\\vec{y}|\\alpha) $ given $p(\\alpha)$ is unknown. \n",
    "\n",
    "Thus, the ML estimate results by finding the solution to\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^k (y_i - s_i(\\alpha)) \\frac{\\partial s_i(\\alpha)}{\\partial \\alpha} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Estimation\n",
    "\n",
    "An MSE estimate can be obtained by computing\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_{\\rm MSE} = \\int_{-\\infty}^{\\infty} \\alpha p(\\alpha | \\vec{y}) d\\alpha\n",
    "$$\n",
    "\n",
    "where the pdf $ p(\\alpha | \\vec{y}) $ is determined by Bayes' rule, i.e.,\n",
    "\n",
    "$$\n",
    "p(\\alpha | \\vec{y}) = \\frac{p(\\vec{y}|\\alpha) p(\\alpha)}{p(\\vec{y})}\n",
    "$$\n",
    "\n",
    "From the condition probability (the A Posteriori), we have\n",
    "\n",
    "$$\n",
    "p(\\alpha | \\vec{y}) = \\kappa p(\\alpha) \\exp \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^k (y_i - s_i(\\alpha))^2 \\right)\n",
    "$$\n",
    "\n",
    "where the constant\n",
    "\n",
    "$$\n",
    "\\kappa \\triangleq \\frac{1}{(2\\pi\\sigma^2)^{k/2} p(\\vec{y})}\n",
    "$$\n",
    "\n",
    "is independent of the parameter $ \\alpha $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous versions of these results can be obtained by utilizing the procedure outlined in Chapter 5 (e.g., multiply with $\\Delta t$). \n",
    "\n",
    "For example, the MAP estimate above can be expressed in continuous form by\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2} \\int_0^T [y(t) - s(t, \\alpha)] \\frac{\\partial s(t, \\alpha)}{\\partial \\alpha} \\, dt + \\frac{\\partial}{\\partial \\alpha} \\ln p(\\alpha) = 0\n",
    "$$\n",
    "\n",
    "where $ T $ is the measurement interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Variance\n",
    "\n",
    "We use the **Cramér-Rao bound (CRB)** to compute the **minimum variance** of an unbiased estimator $ \\hat{\\alpha} $.\n",
    "\n",
    "We have the log-likelihood is:\n",
    "\n",
    "$$\n",
    "\\ln p(\\vec{y}|\\alpha) = -\\frac{k}{2} \\ln 2\\pi \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^k (y_i - s_i(\\alpha))^2\n",
    "$$\n",
    "\n",
    "\n",
    "**Score Function**\n",
    "\n",
    "The score function is the derivative of the log-likelihood with respect to the parameter $ \\alpha $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\alpha} \\ln p(\\vec{y}|\\alpha) = \\frac{1}{\\sigma^2} \\sum_{i=1}^k (y_i - s_i(\\alpha)) \\frac{\\partial s_i(\\alpha)}{\\partial \\alpha}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ \\frac{\\partial s_i(\\alpha)}{\\partial \\alpha} $: Sensitivity of the model output $ s_i $ to changes in the parameter $ \\alpha $.\n",
    "- The term $ (y_i - s_i(\\alpha)) $ captures the residual between the observed data and the model prediction.\n",
    "\n",
    "**Fisher Information**\n",
    "\n",
    "The **Fisher information** is computed as the expectation of the square of the score function:\n",
    "\n",
    "$$\n",
    "E \\left\\{ \\left[ \\frac{\\partial}{\\partial \\alpha} \\ln p(\\vec{y}|\\alpha) \\right]^2 \\right\\}\n",
    "$$\n",
    "\n",
    "Substituting the score function:\n",
    "\n",
    "$$\n",
    "E \\left\\{ \\left[ \\frac{\\partial}{\\partial \\alpha} \\ln p(\\vec{y}|\\alpha) \\right]^2 \\right\\} = \\frac{1}{\\sigma^4} \\sum_{i=1}^k \\sum_{j=1}^k E[n_i n_j] \\frac{\\partial s_i(\\alpha)}{\\partial \\alpha} \\frac{\\partial s_j(\\alpha)}{\\partial \\alpha}\n",
    "$$\n",
    "\n",
    "For **white Gaussian noise**, $ E[n_i n_j] = \\sigma^2 \\delta_{ij} $ (where $ \\delta_{ij} $ is 1 if $ i = j $, and 0 otherwise). This simplifies the double sum to:\n",
    "\n",
    "$$\n",
    "\\mathcal{I}(\\alpha) = \\frac{1}{\\sigma^2} \\sum_{i=1}^k \\left( \\frac{\\partial s_i(\\alpha)}{\\partial \\alpha} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cramér-Rao Bound**\n",
    "\n",
    "The second derivative of the log-likelihood function $ \\ln p(\\vec{y}; \\alpha) $ with respect to $ \\alpha $ is related to the **curvature** of the log-likelihood function. \n",
    "\n",
    "This is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial \\alpha^2} \\ln p(\\vec{y}; \\alpha)\n",
    "$$\n",
    "\n",
    "The log-likelihood function generally has a **maximum** at the true parameter value $ \\alpha $, because it represents the parameter value that maximizes the probability of observing the data $ \\vec{y} $.\n",
    "\n",
    "Near the maximum, the log-likelihood curve bends downwards, which means the second derivative is negative.\n",
    "\n",
    "\n",
    "Thus, The Fisher information is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{I}(\\alpha) = - E\\left\\{\\frac{\\partial^2}{\\partial \\alpha^2} \\ln p(\\vec{y}; \\alpha)\\right\\}\n",
    "$$\n",
    "\n",
    "Here, the negative sign ensures that the Fisher information is **positive**, as the expected value of the second derivative is typically negative.\n",
    "\n",
    "The variance of an unbiased estimator satisfies the inequality:\n",
    "\n",
    "$$\n",
    "V(\\hat{\\alpha}) \\geq \\frac{1}{\\mathcal{I}(\\alpha)}\n",
    "$$\n",
    "\n",
    "where the Fisher information $ \\mathcal{I}(\\alpha) $ is computed as:\n",
    "\n",
    "$$\n",
    "\\mathcal{I}(\\alpha) = -E\\left\\{\\frac{\\partial^2}{\\partial \\alpha^2} \\ln p(\\vec{y}; \\alpha)\\right\\}\n",
    "$$\n",
    "\n",
    "Substituting this into the CRB inequality leads to:\n",
    "\n",
    "$$\n",
    "V(\\hat{\\alpha}) \\geq \\frac{1}{-E\\left\\{\\frac{\\partial^2}{\\partial \\alpha^2} \\ln p(\\vec{y}; \\alpha)\\right\\}}\n",
    "$$\n",
    "\n",
    "which is defined in Chapper 3.\n",
    "\n",
    "**Back to our problem**\n",
    "\n",
    "Substituting $ \\mathcal{I}(\\alpha) $:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\alpha}) \\geq \\frac{\\sigma^2}{\\sum_{i=1}^k \\left( \\frac{\\partial s_i(\\alpha)}{\\partial \\alpha} \\right)^2}\n",
    "$$\n",
    "\n",
    "The **minimum variance** for any unbiased estimator $ \\hat{\\alpha} $, denoted $ \\sigma^2_{\\text{min}}(\\hat{\\alpha}) $, is therefore:\n",
    "\n",
    "$$\n",
    "\\sigma^2_{\\text{min}}(\\hat{\\alpha}) = \\frac{\\sigma^2}{\\sum_{i=1}^k \\left( \\frac{\\partial s_i(\\alpha)}{\\partial \\alpha} \\right)^2}\n",
    "$$\n",
    "\n",
    "**Continuous Case**\n",
    "\n",
    "If the data is continuous over time rather than discrete, the summation is replaced by an integral:\n",
    "\n",
    "$$\n",
    "\\sigma^2_{\\text{min}}(\\hat{\\alpha}) = \\frac{\\sigma^2}{\\int_0^T \\left( \\frac{\\partial s(t, \\alpha)}{\\partial \\alpha} \\right)^2 dt}\n",
    "$$\n",
    "\n",
    "This generalizes the result to cases where $ s(t, \\alpha) $ is a continuous signal."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
