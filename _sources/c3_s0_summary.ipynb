{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Fundamentals of Estimation Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and Properties\n",
    "\n",
    "#### Problem Formulation\n",
    "\n",
    "Estimation problem: estimate a parameter vector $\\vec{\\boldsymbol{\\alpha}}$ from an observation vector $\\vec{\\mathbf{y}}$.\n",
    "\n",
    "\n",
    "Definitions:\n",
    "\n",
    "- **Parameters**: vector of random or deterministic values that need to be estimated.\n",
    "- **Measurements or observations**: vector of random data $\\vec{\\mathbf{y}}$ that depends on the parameters $\\vec{\\boldsymbol{\\alpha}}$ ($p(\\vec{y}|\\vec{\\alpha})$ or $p(\\vec{y};\\vec{\\alpha})$).\n",
    "- **Estimator**: signal processing algorithm $\\vec{\\hat{\\boldsymbol{\\alpha}}}(\\vec{\\mathbf{y}})$ used to calculate an estimate of $\\vec{\\boldsymbol{\\alpha}}$ from the observation vector $\\vec{\\mathbf{y}}$.\n",
    "- **Estimate**: the estimate $\\vec{\\hat{\\boldsymbol{\\alpha}}}$ of $\\vec{\\boldsymbol{\\alpha}}$ is the result of the estimator $\\vec{\\hat{\\boldsymbol{\\alpha}}}(\\vec{\\mathbf{y}})$ for a realization of $\\vec{\\mathbf{y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation of the Single Parameter Estimation Problem\n",
    "\n",
    "In this chapter, we consider the estimation of a single parameter (multi-parameter estimation will be considered later).\n",
    "\n",
    "Definitions:\n",
    "\n",
    "- **Parameter to estimate**: $\\boldsymbol{\\alpha}$\n",
    "- **Observation vector**: $\\vec{\\mathbf{y}}$\n",
    "- **Estimator**: $\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}})$\n",
    "- **Estimate**: $\\hat{\\boldsymbol{\\alpha}}$\n",
    "\n",
    "### Unbiased Estimator\n",
    "\n",
    "- An estimator $\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}})$ for a *deterministic* parameter $\\alpha$ is unbiased if:\n",
    "  $$\n",
    "  E[\\hat{\\boldsymbol{\\alpha}}]=\\alpha, \\forall \\alpha\n",
    "  $$\n",
    "\n",
    "- An estimator $\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}})$ for a *random* parameter $\\boldsymbol{\\alpha}$ is conditionally unbiased if:\n",
    "  $$\n",
    "  E[\\hat{\\boldsymbol{\\alpha}}|\\boldsymbol{\\alpha}=\\alpha]=\\alpha, \\forall \\alpha\n",
    "  $$\n",
    "  and unbiased if:\n",
    "  $$\n",
    "  E[\\hat{\\boldsymbol{\\alpha}}]=E[\\boldsymbol{\\alpha}]\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator with Sufficient Statistic\n",
    "\n",
    "- **Definition**: The estimator $T(\\vec{\\mathbf{y}})$ is a **sufficient statistic** for $\\boldsymbol{\\alpha}$ if it contains all the information about $\\boldsymbol{\\alpha}$ (i.e., no other information can be extracted from $\\vec{\\mathbf{y}}$).\n",
    "- The estimator $T(\\vec{\\mathbf{y}})$ is a sufficient statistic for $\\boldsymbol{\\alpha}$ if\n",
    "  $$\n",
    "  P(\\vec{\\mathbf{y}}|T(\\vec{\\mathbf{y}}),\\boldsymbol{\\alpha})=P(\\vec{\\mathbf{y}}|T(\\vec{\\mathbf{y}}))\n",
    "  $$\n",
    "- One method to demonstrate that $T(\\vec{\\mathbf{y}})$ is a sufficient statistic for $\\boldsymbol{\\alpha}$ is to verify if the following **factorization** is valid:\n",
    "  $$\n",
    "  P(\\vec{\\mathbf{y}}|\\boldsymbol{\\alpha})=g(T(\\vec{\\mathbf{y}}),\\boldsymbol{\\alpha})h(\\vec{\\mathbf{y}})\n",
    "  $$\n",
    "\n",
    "### Minimum Variance Estimaties\n",
    "\n",
    "- Chebyshev's inequality tells us that for an unbiased estimator\n",
    "  $$\n",
    "  P(|\\hat{\\boldsymbol{\\alpha}} - \\alpha| > \\epsilon) < \\frac{V(\\hat{\\boldsymbol{\\alpha}})}{\\epsilon^2}\n",
    "  $$\n",
    "  where $V \\{\\hat{\\boldsymbol{\\alpha}}\\} = E\\{(\\hat{\\boldsymbol{\\alpha}} - \\alpha)^2\\}$.\n",
    "\n",
    "- We desire an estimator with minimal variance.\n",
    "- The CramÃ©r-Rao inequality provides the following *lower bound* for the variance of an estimator of $\\boldsymbol{\\alpha}$:\n",
    "  $$\n",
    "  V(\\hat{\\boldsymbol{\\alpha}}) \\geq \\frac{1}{-E\\left\\{\\frac{\\partial^2}{\\partial\\boldsymbol{\\alpha}^2}\\ln P(\\vec{\\mathbf{y}}|{\\boldsymbol{\\alpha}}) \\right\\}}\n",
    "  $$\n",
    "- An estimate $\\hat{\\boldsymbol{\\alpha}}$ achieving this bound is called the *most efficient* estimate.\n",
    "\n",
    "### Consistent Estimator\n",
    "\n",
    "- The estimator $\\hat{\\boldsymbol{\\alpha}}$ is *consistent* if it converges to $\\alpha$ with probability 1:\n",
    "  $$\n",
    "  \\underset{m \\rightarrow \\infty}{\\lim}\n",
    "  p\\{|\\hat{\\boldsymbol{\\alpha}}(\\vec{y}_m)-\\alpha|\\leq \\epsilon\\}=1,\n",
    "  \\quad \\forall \\epsilon>0\n",
    "  $$\n",
    "  or equivalently:\n",
    "  $$\n",
    "  \\underset{m \\rightarrow \\infty}{\\lim}\n",
    "  p\\{|\\hat{\\boldsymbol{\\alpha}}(\\vec{y}_m)-\\alpha|\\geq \\epsilon\\}=0,\n",
    "  \\quad \\forall \\epsilon>0\n",
    "  $$\n",
    "- Chebyshev's inequality tells us that if\n",
    "  $$\n",
    "  \\underset{m \\rightarrow \\infty}{\\lim} V\\{\\hat{\\boldsymbol{\\alpha}}(\\vec{y}_m)\\}=0\n",
    "  $$\n",
    "  then $\\hat{\\boldsymbol{\\alpha}}$ is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Estimator\n",
    "\n",
    "\n",
    "- Similar to the detection problem, we define the **average cost** or **risk**:\n",
    "  $$\n",
    "  \\mathcal{R}=E\\{C(\\boldsymbol{\\alpha},\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}}))\\}=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}C(\\alpha,\\hat{\\alpha}(\\vec{y}))p(\\alpha,\\vec{y})d\\alpha d\\vec{y}\n",
    "  $$\n",
    "  where $C(\\boldsymbol{\\alpha},\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}}))$ is a **cost function** introduced by the estimation error $\\mathbf{\\alpha}_e=\\alpha-\\hat{\\alpha}(\\vec{y})$.\n",
    "\n",
    "- The Bayes estimator is the estimator $\\hat{\\boldsymbol{\\alpha}}_{\\text{B}}(\\vec{\\mathbf{y}})$ that *minimizes* $\\mathcal{R}$.\n",
    "\n",
    "- It can be shown that the Bayes estimator minimizes the conditional cost and is therefore given by:\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\alpha}}_{\\text{B}}(\\vec{\\mathbf{y}})=\\arg\\underset{\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}})}{\\min}E\\{C(\\boldsymbol{\\alpha},\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}}))\\} \\\\\n",
    "\n",
    "  =\\arg\\underset{\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}})}{\\min}E\\{C(\\boldsymbol{\\alpha},\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}}))|\\vec{y}\\} \\\\\n",
    "\n",
    "  =\\arg\\underset{\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}})}{\\min}\\int_{-\\infty}^{\\infty}C(\\alpha,\\hat{\\alpha}(\\vec{y}))p(\\alpha|\\vec{y})d\\alpha\n",
    "  $$\n",
    "\n",
    "\n",
    "Typical examples of cost functions:\n",
    "\n",
    "1. **Quadratic Error**:\n",
    "   $$\n",
    "   C_{\\text{S}} (\\alpha,\\hat{\\alpha}(\\vec{y}))=\\alpha_e^2=(\\alpha-\\hat{\\alpha}(\\vec{y}))^2\n",
    "   $$\n",
    "2. **Uniform Error** (or \"hit or miss\"):\n",
    "   $$\n",
    "   C_{\\text{U}} (\\alpha,\\hat{\\alpha}(\\vec{y}))=\\begin{cases}\n",
    "   0, & |\\alpha_e|<\\Delta/2 \\\\ \n",
    "   1, & |\\alpha_e|>\\Delta/2 \n",
    "   \\end{cases}\n",
    "   $$\n",
    "3. **Absolute Error**:\n",
    "   $$\n",
    "   C_{\\text{A}} (\\alpha,\\hat{\\alpha}(\\vec{y}))=|\\alpha_e|\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### MSE Estimator\n",
    "\n",
    "- The Mean Squared Error (MSE) estimator is the Bayes estimator with the *quadratic* error cost function and is given by:\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\alpha}}_{\\text{MSE}}(\\vec{\\mathbf{y}})=\\arg\\underset{\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}})}{\\min}\\int_{-\\infty}^{\\infty}(\\alpha-\\hat{\\alpha}(\\vec{y}))^2p(\\alpha|\\vec{y})d\\alpha\n",
    "  $$\n",
    "\n",
    "- By differentiation, we obtain:\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\alpha}}_{\\text{MSE}}(\\vec{\\mathbf{y}})=\\int_{-\\infty}^{\\infty}\\alpha p(\\alpha|\\vec{y})d\\alpha=E\\{\\boldsymbol{\\alpha}|\\vec{y}\\}\n",
    "  $$\n",
    "\n",
    "- Note: In the case where *no* observation is available, we obtain $\\hat{\\boldsymbol{\\alpha}}_{\\text{MSE}}=E\\{\\boldsymbol{\\alpha}\\}$.\n",
    "\n",
    "### MAP Estimator\n",
    "\n",
    "- The Maximum A Posteriori (MAP) estimator is obtained by maximizing the posterior probability (the a posteriori probability) of the parameter to be estimated:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\alpha}}_{\\text{MAP}}(\\vec{\\mathbf{y}})=\\arg\\underset{\\boldsymbol{\\alpha}(\\vec{\\mathbf{y}})}{\\max}\\{p(\\alpha|\\vec{y})\\} \\\\\n",
    " \n",
    "  =\\arg\\underset{\\boldsymbol{\\alpha}(\\vec{\\mathbf{y}})}{\\max}\\{p(\\vec{y}|\\alpha)p(\\alpha)\\}\n",
    "  $$\n",
    "\n",
    "- It can be shown that the MAP estimator is actually a Bayes estimator with the *uniform* error cost function.\n",
    "\n",
    "- In practice, if $p(\\alpha|\\vec{y})$ is differentiable, then $\\hat{\\boldsymbol{\\alpha}}_{\\text{MAP}}(\\vec{\\mathbf{y}})$ is the solution of\n",
    "  $$\n",
    "  \\frac{\\partial}{\\partial\\alpha}p(\\alpha|\\vec{y})= 0 \\text{ or }\n",
    "  \\frac{\\partial}{\\partial\\alpha}p(\\vec{y}|\\alpha)p(\\alpha)=0\n",
    "  $$\n",
    "  or\n",
    "  $$\n",
    "  \\frac{\\partial}{\\partial\\alpha}\\ln p(\\alpha|\\vec{y})=0  \\text{ or } \\frac{\\partial}{\\partial\\alpha}\\ln p(\\vec{y}|\\alpha)+\\frac{\\partial}{\\partial\\alpha}\\ln p(\\alpha)=0\n",
    "  $$\n",
    "\n",
    "### ML Estimator\n",
    "\n",
    "- The Maximum Likelihood (ML) estimator is obtained by maximizing the likelihood function:\n",
    "  $$\n",
    "  \\hat{\\boldsymbol{\\alpha}}_{\\text{ML}}(\\vec{\\mathbf{y}})=\\arg\\underset{\\boldsymbol{\\alpha}(\\vec{\\mathbf{y}})}{\\max}\\{p(\\vec{y}|\\alpha)\\}\n",
    "  $$\n",
    "\n",
    "- In practice, if $p(\\vec{y}|\\alpha)$ is differentiable, then $\\hat{\\boldsymbol{\\alpha}}_{\\text{ML}}(\\vec{\\mathbf{y}})$ is the solution of\n",
    "  $$\n",
    "  \\frac{\\partial}{\\partial\\alpha}p(\\vec{y}|\\alpha)=0\n",
    "  $$\n",
    "  or\n",
    "  $$\n",
    "  \\frac{\\partial}{\\partial\\alpha}\\ln p(\\vec{y}|\\alpha)=0\n",
    "  $$\n",
    "\n",
    "- Note: The ML estimator corresponds to the MAP estimator when $\\boldsymbol{\\alpha}$ is very *dispersed* (i.e., uniform distribution)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
