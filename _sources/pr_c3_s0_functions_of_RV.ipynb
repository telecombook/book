{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of A Random Variable\n",
    "\n",
    "### Expectation\n",
    "\n",
    "**Expectation Overview**  \n",
    "The probability density function (PDF), $ f_X(x) $, provides a complete statistical description of a continuous random variable (RV), $ X $. While this level of detail is comprehensive, it often exceeds practical requirements. For most real-world scenarios, simpler measures, such as statistical averages, provide sufficient information for characterizing the behavior of $ X $. Among these, the focus is placed on the **first-order average**, known as the expected value or mean, due to its practical relevance. Higher-order averages, such as variance and covariance, are explored separately.\n",
    "\n",
    "**Definition of the Mean**  \n",
    "The expected value or mean of a continuous random variable $ X $ is a measure of its central tendency. It is mathematically defined as:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mu_X = \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x) dx\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\mu_X $ represents the mean.\n",
    "- $ \\mathbb{E}[X] $ denotes the expectation or averaging operator.\n",
    "- $ f_X(x) $ is the PDF of $ X $. \n",
    "\n",
    "The mean (or expected value) serves as a foundational concept in probability and statistics due to its wide application in summarizing data and modeling real-world phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notations for Expected Value:\n",
    "\n",
    "The terms **average**, **mean**, **expectation**, and **first moment** are synonymous and represent the same concept of **expected value**. These terms will be used interchangeably, as they all describe the measure of the central tendency of a random variable (RV).\n",
    "\n",
    "\n",
    "1. **Common Notations**:\n",
    "   - $ \\mathbb{E}[X] $: The expectation operator applied to a random variable $ X $.\n",
    "   - $ \\mu_X $: The mean or expected value of $ X $.\n",
    "   - $ \\overline{X} $: Another notation indicating the mean or expected value.\n",
    "\n",
    "2. **Consistency Across Representations**:  \n",
    "   Each of these notations communicates the same concept but may be used in different contexts for clarity, convenience, or alignment with specific conventions.\n",
    "\n",
    "#### Interpretation as Function of the PDF\n",
    "\n",
    "The **expectation operator** $ \\mathbb{E} $, when applied to a continuous random variable $ X $, yields a **unique scalar value**. This value is computed using the probability density function $ f_X(x) $ of $ X $, as shown by:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\int_{-\\infty}^\\infty x f_X(x) dx\n",
    "$$\n",
    "\n",
    "This highlights that the expected value is inherently tied to the distribution of the random variable and summarizes its central location based on the probabilities of different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation for Discrete Random Variables\n",
    "\n",
    "For a discrete random variable (RV), the probability density function (PDF) is expressed in terms of the **probability mass function (PMF)** using the delta function:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\sum_{k} p_X(x_k) \\delta(x - x_k),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ p_X(x_k) $ represents the PMF, assigning probabilities to the discrete values $ x_k $.\n",
    "- $ \\delta(x - x_k) $ is the delta function that isolates contributions at the specific points $ x_k $.\n",
    "\n",
    "**Definition of Expected Value for Discrete RVs**  \n",
    "The expected value, or mean, of a discrete RV $ X $, is defined as:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbb{E}[X] = \\sum_{k} x_k p_X(x_k),\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ x_k $: Possible values of the RV $ X $.\n",
    "- $ p_X(x_k) $: The probability mass associated with $ x_k $.\n",
    "\n",
    "We can interpret this definition as the expected value of a discrete RV is a **weighted average** of its possible values, with the probabilities $ p_X(x_k) $ serving as the weights. Each $ x_k $ contributes to the expected value proportionally to how likely it is to occur.\n",
    "\n",
    "**Notes on the Existence of Expected Value**  \n",
    "The expected value of a random variable exists only if the summation $ \\sum_{k} x_k p_X(x_k) $ converges, meaning the series does not diverge. This depends on the specific values of $ x_k $ and their associated probabilities $ p_X(x_k) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the Expectation Operator\n",
    "\n",
    "#### Property 1: Linearity of Expectation\n",
    "\n",
    "The **linearity property** of the expectation operator states that the expected value of the sum of random variables (RVs) is equal to the sum of their individual expected values. Mathematically, this is expressed as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X + Y] = \\mathbb{E}[X] + \\mathbb{E}[Y]\n",
    "$$\n",
    "\n",
    "##### Extension to Multiple Random Variables\n",
    "\n",
    "This property can be generalized to the sum of multiple random variables using the principle of induction:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbb{E}\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_i]\n",
    "}\n",
    "$$\n",
    "\n",
    "This interpret that the **expectation operator distributes over addition**, making it a highly convenient tool in statistical analysis. It can be interpret in simple terms:\n",
    "\n",
    "*The expectation of a sum of RVs is equal to the sum of their individual expectations.*\n",
    "\n",
    "This property is valid for both discrete and continuous random variables, regardless of whether the variables are dependent or independent. It facilitates a number of statistical and probabilistic analyses, such as finding the mean of a combined random process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property 2: Statistical Independence\n",
    "\n",
    "The **statistical independence** of random variables (RVs) leads to a simplification of the expectation of their product. If $ X $ and $ Y $ are independent, the expectation of their product $ Z = XY $ is given by:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y].\n",
    "$$\n",
    "\n",
    "##### Generalization to Multiple Random Variables\n",
    "\n",
    "This property extends to the product of multiple independent random variables. By induction, for independent $ X_1, X_2, \\dots, X_n $, we have:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbb{E}\\left[\\prod_{i=1}^{n} X_i\\right] = \\prod_{i=1}^{n} \\mathbb{E}[X_i].\n",
    "}\n",
    "$$\n",
    "\n",
    "We can interpret this property as the **expectation of the product of independent RVs equals the product of their individual expectations**. \n",
    "\n",
    "This result is foundational in probability theory and has wide applications, particularly in communication systems and machine learning, where independence assumptions often simplify computations.\n",
    "\n",
    "##### Independence Condition\n",
    "\n",
    "The property holds **only when the random variables are statistically independent**, meaning the joint probability distribution of $ X_1, X_2, \\dots, X_n $ can be factored into the product of their individual distributions:\n",
    "\n",
    "$$\n",
    "\\Pr(X_1, X_2, \\dots, X_n) = \\prod_{i=1}^{n} \\Pr(X_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Values of Functions of Random Variables\n",
    "\n",
    "**Definition**  \n",
    "For a random variable (RV) $ X $, the expected value of a function $ g(X) $ is a generalization of the concept of expectation. It represents the average or mean value of the function $ g(X) $ weighted by the probability distribution of $ X $.\n",
    "\n",
    "**For Continuous RVs**  \n",
    "If $ X $ is a continuous RV with probability density function (PDF) $ f_X(x) $, the expected value of $ g(X) $ is given by:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx.\n",
    "}\n",
    "$$\n",
    "\n",
    "**For Discrete RVs**  \n",
    "If $ X $ is a discrete RV with probability mass function (PMF) $ p_X(x_k) $, the expected value of $ g(X) $ becomes:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[g(X)] = \\sum_{k} g(x_k) p_X(x_k).\n",
    "$$\n",
    "\n",
    "We can see that the expectation $ \\mathbb{E}[g(X)] $ calculates the *weighted average* of the function $ g(X) $, where the *weights* correspond to the probabilities of $ X $. \n",
    "\n",
    "This generalized expectation forms the basis for analyzing transformations of random variables and for deriving key statistical measures, such as variance, moments, and covariance, by appropriately choosing $ g(X) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity of Expectation\n",
    "The expectation operator is inherently **linear**, meaning it satisfies specific properties when applied to linear combinations of random variables or functions.\n",
    "\n",
    "**Theorem: Expectation of a Linear Function**  \n",
    "For any constants $ a $ and $ b $, the expected value of a linear transformation $ aX + b $ is:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbb{E}[aX + b] = a\\mathbb{E}[X] + b.\n",
    "}\n",
    "$$\n",
    "\n",
    "where\n",
    "- The constant $ a $ scales the expectation of $ X $.\n",
    "- The constant $ b $ adds a fixed value to the result, reflecting the shift in the distribution.\n",
    "\n",
    "**Expectation of a Sum of Functions**  \n",
    "If a function $ g(x) $ can be expressed as a sum of $ N $ component functions, $ g(x) = g_1(x) + g_2(x) + \\dots + g_N(x) $, then:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\sum_{k=1}^{N} g_k(X)\\right] = \\sum_{k=1}^{N} \\mathbb{E}[g_k(X)].\n",
    "$$\n",
    "\n",
    "This property extends the linearity of expectation to sums of arbitrary functions of the random variable $ X $.\n",
    "\n",
    "We can see that expectation is a **linear operation**, meaning it can be interchanged with addition and scalar multiplication. This simplifies computations and forms the foundation for many probabilistic and statistical analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moments\n",
    "\n",
    "**Definition of Moments**  \n",
    "The **$ n $-th moment** of a random variable (RV) $ X $ provides a measure of the distribution's shape by considering powers of $ X $. \n",
    "\n",
    "Moments are mathematically defined as follows:\n",
    "\n",
    "- **For Continuous Random Variables**\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[X^n] = \\int_{-\\infty}^{\\infty} x^n f_X(x) dx,\n",
    "   $$\n",
    "\n",
    "   where $ f_X(x) $ is the probability density function (PDF) of $ X $.\n",
    "\n",
    "- **For Discrete Random Variables**\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[X^n] = \\sum_{k} x_k^n p_X(x_k),\n",
    "   $$\n",
    "\n",
    "   where $ p_X(x_k) $ is the probability mass function (PMF).\n",
    "\n",
    "#### Zeroth Moment\n",
    "The **zeroth moment** represents the **total area under the PDF**, which must equal 1 for any valid probability distribution:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^0] = \\int_{-\\infty}^{\\infty} f_X(x) dx = 1.\n",
    "$$\n",
    "\n",
    "#### Commonly Used Moments\n",
    "\n",
    "1. **First Moment (Mean)**:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[X] = \\mu_X.\n",
    "   $$\n",
    "\n",
    "   - Represents the **central tendency** of the distribution.\n",
    "   - For symmetric distributions, like noise centered around zero, the mean is zero, indicating no bias.\n",
    "\n",
    "2. **Second Moment (Mean Squared Value)**:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[X^2].\n",
    "   $$\n",
    "\n",
    "   - Measures the **average squared value** of $ X $.\n",
    "   - For noise, this provides a measure of its **strength** or **energy**.\n",
    "\n",
    "**Example: Noise Distribution**  \n",
    "If $ X $ represents a noise waveform:\n",
    "- A **zero mean** ($ \\mathbb{E}[X] = 0 $) indicates that the noise is symmetric and unbiased.\n",
    "- The **second moment** ($ \\mathbb{E}[X^2] $) quantifies the noise's strength or power.\n",
    "\n",
    "It is noted that moments are critical in characterizing distributions:\n",
    "- The **first moment** describes the location (mean).\n",
    "- The **second moment** relates to variability or spread (variance is derived from it).\n",
    "- Higher-order moments provide insights into *skewness* and *kurtosis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Moments\n",
    "\n",
    "Central moments help characterize the variability or randomness of a random variable (RV) more effectively, especially in cases where the RV $ Y $ combines a deterministic part $ a $ and a random part $ X $, such as:\n",
    "\n",
    "$$\n",
    "Y = a + X.\n",
    "$$\n",
    "\n",
    "If the random part $ X $ is small compared to the deterministic part $ a $, the moments of $ Y $ are dominated by the fixed part ($ a $). In such cases, central moments are used to focus on the random fluctuations by subtracting the mean.\n",
    "\n",
    "**Definition of Central Moments**  \n",
    "The $ n $-th central moment of a random variable $ X $ is defined as:\n",
    "- **For Continuous RVs**:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[(X - \\mu_X)^n] = \\int_{-\\infty}^{\\infty} (x - \\mu_X)^n f_X(x) dx,\n",
    "   $$\n",
    "\n",
    "   where $ \\mu_X $ is the mean ($ \\mathbb{E}[X] $) of $ X $.\n",
    "\n",
    "- **For Discrete RVs**:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[(X - \\mu_X)^n] = \\sum_{k} (x_k - \\mu_X)^n p_X(x_k),\n",
    "   $$\n",
    "\n",
    "   where $ p_X(x_k) $ is the probability mass function (PMF) of $ X $.\n",
    "\n",
    "This can be interpreted as:\n",
    "- **Subtracting the Mean**: By subtracting $ \\mu_X $ (the mean), central moments remove the bias introduced by the location of the distribution. This ensures that the higher moments reflect variability around the mean.\n",
    "\n",
    "- **Zeroth Central Moment**: The zeroth central moment equals the total probability, which is always 1 for a valid probability distribution.\n",
    "\n",
    "- **Higher-Order Central Moments**:\n",
    "  - The **second central moment** ($ \\mathbb{E}[(X - \\mu_X)^2] $) is the **variance**, a measure of the spread or dispersion of the distribution.\n",
    "  - Higher-order central moments provide insights into features like skewness (asymmetry) and kurtosis (peakedness).\n",
    "\n",
    "Central moments are particularly useful when studying distributions where the mean does not adequately capture the randomness or variability. They are fundamental in noise analysis, signal processing, and statistical characterization of *random processes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Expected Values\n",
    "\n",
    "**Definition**  \n",
    "The **conditional expected value** of a random variable (RV) provides the average value of the RV under the condition that a specific event $ A $ has occurred. It adjusts the expectation by weighting the possible values of the RV based on the **conditional probability distribution**.\n",
    "\n",
    "- **For Continuous Random Variables**:\n",
    "   If $ X $ is a continuous RV, the conditional expected value is:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[X|A] = \\int_{-\\infty}^{\\infty} x f_{X|A}(x) dx,\n",
    "   $$\n",
    "\n",
    "   where $ f_{X|A}(x) $ is the **conditional probability density function (PDF)** of $ X $, given $ A $.\n",
    "\n",
    "- **For Discrete Random Variables**:\n",
    "   If $ X $ is a discrete RV, the conditional expected value is:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[X|A] = \\sum_{k} x_k p_{X|A}(x_k),\n",
    "   $$\n",
    "\n",
    "   where $ p_{X|A}(x_k) $ is the **conditional probability mass function (PMF)** of $ X $, given $ A $.\n",
    "\n",
    "#### Conditional Expectation of a Function of a RV\n",
    "\n",
    "The conditional expectation extends naturally to functions of a random variable. For a function $ g(X) $:\n",
    "- **Continuous RV**:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[g(X)|A] = \\int_{-\\infty}^{\\infty} g(x) f_{X|A}(x) dx.\n",
    "  $$\n",
    "\n",
    "- **Discrete RV**:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[g(X)|A] = \\sum_{k} g(x_k) p_{X|A}(x_k).\n",
    "  $$\n",
    "\n",
    "It is noted that the conditional expectation is computed similarly to regular expectation but uses the **conditional PDF or PMF** instead of the marginal distributions. It effectively recalculates the average value based on the knowledge that event $ A $ has occurred.\n",
    "\n",
    "Main applications are:\n",
    "- **Probabilistic Modeling**: Useful for updating predictions when new information (event $ A $) is available.\n",
    "- **Bayesian Inference**: Forms the basis of posterior expectations.\n",
    "- **Signal Processing**: Helps in filtering and prediction of signals with known prior events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristic Functions\n",
    "\n",
    "The **characteristic function** of a random variable (RV) is a mathematical tool closely related to the **Fourier transform** of the probability density function (PDF). It provides a **frequency-domain representation** of the RV, offering an alternative perspective on its statistical properties.\n",
    "\n",
    "**Definition**  \n",
    "The characteristic function of a random variable $ X $ is defined as:\n",
    "\n",
    "$$\n",
    "\\Phi_X(\\omega) = \\mathbb{E}[e^{j\\omega X}] = \\int_{-\\infty}^{\\infty} e^{j\\omega x} f_X(x) dx,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\omega $ is the frequency variable.\n",
    "- $ f_X(x) $ is the PDF of $ X $.\n",
    "- $ \\mathbb{E}[\\cdot] $ represents the expectation operator.\n",
    "\n",
    "We can see that:\n",
    "- **Connection to Fourier Transform**:\n",
    "   - The characteristic function $ \\Phi_X(\\omega) $ resembles the Fourier transform of the PDF $ f_X(x) $.\n",
    "   - In electrical engineering literature, the Fourier transform of $ f_X(x) $ is typically expressed as $ \\Phi_X(-\\omega) $.\n",
    "\n",
    "- **Inverse Relationship**:\n",
    "   - The PDF $ f_X(x) $ can be recovered from its characteristic function $ \\Phi_X(\\omega) $ using the **inverse Fourier transform**:\n",
    "\n",
    "     $$\n",
    "     f_X(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-j\\omega x} \\Phi_X(\\omega) d\\omega.\n",
    "     $$\n",
    "\n",
    "#### Use Cases\n",
    "- **Analysis of RVs**:\n",
    "   - Characteristic functions are useful for analyzing the properties of random variables, such as moments and distributions.\n",
    "\n",
    "- **Simplifying Operations**:\n",
    "   - Operations such as sums of independent random variables become straightforward in the frequency domain because the characteristic functions of the summed variables multiply.\n",
    "\n",
    "- **Frequency-Domain Perspective**:\n",
    "   - While $ \\omega $ does not represent a physical frequency, the frequency-domain representation provides insights into the distribution’s behavior, such as its spread and symmetry.\n",
    "\n",
    "In short, characteristic functions offer a Fourier-based approach to analyzing random variables, providing a bridge between the time-domain representation (PDF) and the frequency domain. Through the inverse Fourier transform, all information about the random variable’s PDF is preserved and can be recovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Generating Functions\n",
    "\n",
    "**Connection to Signal Analysis**  \n",
    "- In signal analysis, **Fourier transforms** are widely used for analyzing continuous-time signals, while the **z-transform** is the standard tool for discrete-time signals.\n",
    "- Similarly, in probability theory:\n",
    "  - The **characteristic function** serves as a Fourier-like tool for continuous random variables (RVs).\n",
    "  - The **probability generating function (PGF)** plays an analogous role for discrete RVs, offering a convenient way to work with their distributions.\n",
    "\n",
    "**Definition of PGF**  \n",
    "The **probability generating function (PGF)** of a discrete random variable $ X $ with a probability mass function (PMF) $ p_X(k) $, defined for nonnegative integers $ k = 0, 1, 2, \\dots $, is given by:\n",
    "\n",
    "$$\n",
    "H_X(z) = \\sum_{k=0}^\\infty p_X(k) z^k.\n",
    "$$\n",
    "\n",
    "We can see that this formulation highlights a direct resemblance to the **unilateral z-transform**, as both involve representing functions in terms of powers of $ z $.\n",
    "\n",
    "#### Deriving the Mean Using the PGF\n",
    "\n",
    "The **mean** (expected value) of a discrete RV $ X $ can be obtained from the first derivative of its PGF, evaluated at $ z = 1 $:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\left. \\frac{d}{dz} H_X(z) \\right|_{z=1}.\n",
    "$$\n",
    "\n",
    "#### Higher-Order Derivatives and Factorial Moments\n",
    "\n",
    "The **factorial moments** of a discrete RV $ X $ are derived from the higher-order derivatives of the PGF, evaluated at $ z = 1 $:\n",
    "\n",
    "$$\n",
    "h_k = \\left. \\frac{d^k}{dz^k} H_X(z) \\right|_{z=1} = \\mathbb{E}[X(X - 1)(X - 2) \\cdots (X - k + 1)].\n",
    "$$\n",
    "\n",
    "\n",
    "**Proof**. Differentiating $k$ times:\n",
    "\n",
    "$$\n",
    "\\frac{d^k}{dz^k} H_X(z) = \\sum_{n=k}^\\infty \\frac{n!}{(n-k)!} p_X(n) z^{n-k}.\n",
    "$$\n",
    "\n",
    "At $z = 1$, the result simplifies to:\n",
    "\n",
    "$$\n",
    "h_k = \\sum_{n=k}^\\infty \\frac{n!}{(n-k)!} p_X(n),\n",
    "$$\n",
    "\n",
    "which is equivalent to $\\mathbb{E}[X(X-1)\\cdots(X-k+1)]$. $\\hspace{1em} \\blacksquare$\n",
    "  \n",
    "These factorial moments are useful for analyzing higher-order properties of the distribution, such as variability and dispersion.\n",
    "\n",
    "**Applications**\n",
    "- The PGF provides a tool to obtain key statistical measures, such as the mean and higher-order moments, in a compact and systematic form.\n",
    "- The PGF provides a systematic way to analyze discrete random variables, e.g., *the number of successful transmissions* in a wireless system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment Generating Functions\n",
    "\n",
    "**Laplace Transform and Moment Generating Functions**  \n",
    "Many real-world random quantities are **nonnegative**, such as:\n",
    "- Frequency of a random signal.\n",
    "- Time intervals (e.g., between arrivals in a queue).\n",
    "- Nonnegative outcomes like scores in a game.\n",
    "\n",
    "For these **one-sided distributions**, the **Laplace transform** is a standard tool in signal analysis, and the **moment generating function (MGF)** serves as its equivalent in probability theory.\n",
    "\n",
    "**Definition of MGF**  \n",
    "The **moment generating function** of a nonnegative random variable $ X $ is defined as:\n",
    "\n",
    "$$\n",
    "M_X(u) = \\mathbb{E}[e^{uX}] = \\int_{0}^\\infty f_X(x) e^{ux} dx,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ M_X(u) $: The MGF.\n",
    "- $ f_X(x) $: The probability density function (PDF) of $ X $.\n",
    "\n",
    "The MGF resembles the **Laplace transform** of the PDF, providing a frequency-domain representation for random variables.\n",
    "\n",
    "#### Derive the PDF from the MGF\n",
    "\n",
    "The PDF can, in principle, be recovered from the MGF using an operation analogous to an **inverse Laplace transform**:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\frac{1}{2\\pi j} \\int_{c-j\\infty}^{c+j\\infty} M_X(u)e^{-ux} du.\n",
    "$$\n",
    "\n",
    "The integral is computed along the **Bromwich contour**, which must be to the left of all poles of the MGF due to the sign convention in the exponential term.\n",
    "\n",
    "#### Moments from the MGF\n",
    "\n",
    "The **moments** of the random variable $ X $ are derived from the derivatives of the MGF, evaluated at $ u = 0 $:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^k] = \\left. \\frac{d^k}{du^k} M_X(u) \\right|_{u=0}.\n",
    "$$\n",
    "\n",
    "This property gives the MGF its name, as it directly generates the moments of $ X $.\n",
    "\n",
    "**Applications**  \n",
    "The MGF approach is a mathematical technique commonly used in communication theory to evaluate the average error probability in digital communication systems.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
