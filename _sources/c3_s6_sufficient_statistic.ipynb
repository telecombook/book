{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators Based on Sufficient Statistics\n",
    "\n",
    "### Statistic\n",
    "\n",
    "Let $ T(\\vec{\\mathbf{y}}) $ be a **statistic**, which is a function of the observed random variables $ \\mathbf{y}_1, \\mathbf{y}_2, \\ldots, \\mathbf{y}_m $ generated by a probability density function (PDF) $ p(y_1, y_2, \\ldots, y_m; \\alpha) $ where $\\alpha$ is an unknown parameter. \n",
    "\n",
    "Note that\n",
    "- When random samples depend on $\\alpha$, the statistic derived from these samples is used to infer $\\alpha$. \n",
    "- A useful statistic should summarize all the information from the measurements efficiently, often by reducing multiple random variables to a single manageable form. \n",
    "- Such a statistic can serve as a good estimator of $\\alpha$. \n",
    "\n",
    "For instance:\n",
    "- The estimator $\\hat{\\boldsymbol{\\alpha}} = \\mathbf{y}_1$ does not capture all the information about $\\alpha$. \n",
    "- However, the sample mean-based estimator $\\hat{\\boldsymbol{\\alpha}} = T(\\vec{\\mathbf{y}}) \\triangleq \\bar{\\mathbf{y}}$, derived from independent, identically distributed normal random variables, retains all relevant information about $\\alpha$ and is considered a *sufficient* statistic.\n",
    "- This implies that $ T(\\vec{\\mathbf{y}}) = \\bar{\\mathbf{y}} $ encapsulates all the information about $\\boldsymbol{\\alpha}$ available in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Sufficient Statistic\n",
    "\n",
    "In the context of statistical estimation, a **sufficient statistic** is a function of the data that captures all the information necessary to estimate a parameter.\n",
    "\n",
    "Mathematically speaking, a statistic $ T(\\vec{\\mathbf{y}}) $ is considered sufficient for $\\alpha$ if the conditional pdf of the data given $ T(\\vec{\\mathbf{y}})$, i.e., $p(y_1, y_2, \\ldots, y_m \\mid T(\\vec{y}))$ does not depend on $\\alpha$. \n",
    "\n",
    "Formally, if you have a set of observations $\\vec{\\mathbf{y}} = (\\mathbf{y}_1, \\mathbf{y}_2, \\ldots, \\mathbf{y}_m)$ and a parameter of interest $\\boldsymbol{\\alpha}$, then a statistic $T(\\vec{\\mathbf{y}})$ is said to be sufficient for $\\boldsymbol{\\alpha}$ if the conditional distribution of the data $\\vec{\\mathbf{y}}$, given the statistic $T(\\vec{\\mathbf{y}})$ and the parameter $\\boldsymbol{\\alpha}$, is independent of $\\boldsymbol{\\alpha}$, i.e.:\n",
    "\n",
    "$$ \\color{blue}\n",
    "P(\\vec{\\mathbf{y}}|T(\\vec{\\mathbf{y}}),\\boldsymbol{\\alpha}) = P(\\vec{\\mathbf{y}}|T(\\vec{\\mathbf{y}}))\n",
    "$$\n",
    "\n",
    "This means that once you know $T(\\vec{\\mathbf{y}})$, the original data $\\vec{\\mathbf{y}}$ provides no additional information about the parameter $\\boldsymbol{\\alpha}$. In other words, $T(\\vec{\\mathbf{y}})$ captures all the information that $\\vec{\\mathbf{y}}$ contains about $\\boldsymbol{\\alpha}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine If a Statistic is Sufficient\n",
    "\n",
    "One method to determine whether a statistic is sufficient, e.g., a statistic $ T(\\bar{\\mathbf{y}}) $ is sufficient for a parameter $\\alpha$, is through the **Fisher Factorization Theorem**. \n",
    "\n",
    "According to the theorem, if the probability density function (pdf) $ p(y_1, y_2, \\ldots, y_m; \\alpha) $ can be factored into two parts:\n",
    "\n",
    "- $ g(T(\\bar{\\vec{y}}), \\alpha) $: a function that depends on the statistic $ T(\\bar{\\mathbf{y}}) $ and the parameter $\\alpha$.\n",
    "- $ h(y_1, y_2, \\ldots, y_m) $: a function that depends only on the observed data and not on the parameter $\\alpha$.\n",
    "\n",
    "Then, $ T(\\bar{\\vec{y}}) $ is a sufficient statistic for $\\alpha$. \n",
    "\n",
    "Mathematically, the factorization can be expressed as:\n",
    "\n",
    "$$ \\color{blue}\n",
    "p(y_1, \\ldots, y_m; \\alpha) = g(T(y_1, \\ldots, y_m), \\alpha) h(y_1, \\ldots, y_m)\n",
    "$$\n",
    "\n",
    "Here, $ g(\\cdot) $ encapsulates the dependency on the statistic and the parameter, while $ h(\\cdot) $ isolates the data dependency, confirming the sufficiency of $ T(\\bar{\\mathbf{y}}) $.\n",
    "\n",
    "Therefore, an estimate (or statistic) $ T(\\bar{\\mathbf{y}}) $ is considered a *sufficient estimate* for a parameter $\\alpha$ if it satisfies the Fisher factorization theorem. \n",
    "\n",
    "*The converse is also true*. If $ T(\\vec{y}) $ is a sufficient statistic for the parameter $\\alpha$, then the probability density function (pdf) $ p(\\vec{y}; \\alpha) $ can be factored according to the Fisher factorization theorem. \n",
    "\n",
    "This means that if $ T(\\vec{y}) $ is sufficient, the pdf can be expressed as:\n",
    "\n",
    "$$\n",
    "p(\\vec{y}; \\alpha) = g(T(\\vec{y}), \\alpha) \\, h(\\vec{y})\n",
    "$$\n",
    "\n",
    "\n",
    "In general, $T(\\vec{\\mathbf{y}})$ is a sufficient statistic for $\\boldsymbol{\\alpha}$ if and only if the probability distribution (or likelihood) $p(\\vec{\\mathbf{y}}|\\boldsymbol{\\alpha})$ can be factored into the product of two functions:\n",
    "\n",
    "$$ \\color{blue}\n",
    "p(\\vec{\\mathbf{y}}|\\boldsymbol{\\alpha}) = g(T(\\vec{\\mathbf{y}}), \\boldsymbol{\\alpha}) h(\\vec{\\mathbf{y}})\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $g(T(\\vec{\\mathbf{y}}), \\boldsymbol{\\alpha})$ is a function that depends on the data only through the statistic $T(\\vec{\\mathbf{y}})$ and the parameter $\\boldsymbol{\\alpha}$, \n",
    "    - It encapsulates all the information about the parameter $\\boldsymbol{\\alpha}$ through the statistic $T(\\vec{\\mathbf{y}})$.\n",
    "- $h(\\vec{\\mathbf{y}})$ is a function that depends on the data $\\vec{\\mathbf{y}}$ but not on the parameter $\\boldsymbol{\\alpha}$. \n",
    "    - It does not provide any information about $\\boldsymbol{\\alpha}$.\n",
    "    - It only contributes to the overall likelihood through the data distribution, not through the parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Sample Mean as a Sufficient Statistic\n",
    "\n",
    "Consider the scenario where you have a set of $m$ independent and identically distributed (i.i.d.) observations $\\vec{\\mathbf{y}} = (\\mathbf{y}_1, \\mathbf{y}_2, \\ldots, \\mathbf{y}_m)$, each RV follows a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, where the mean $\\mu$ is the parameter of interest, and $\\sigma^2$ is known.\n",
    "\n",
    "The likelihood function is:\n",
    "\n",
    "$$\n",
    "p(\\vec{y};\\mu, \\sigma^2) = \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$\n",
    "p(y_1, \\ldots, y_m; \\mu, \\sigma^2) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^m \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{m} (y_i - \\mu)^2\\right)\n",
    "$$\n",
    "\n",
    "The sum of squares is expanded as\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{m} (y_i - \\mu)^2 = \\sum_{i=1}^{m} y_i^2 - 2\\mu \\sum_{i=1}^{m} y_i + m\\mu^2\n",
    "$$\n",
    "\n",
    "The sample mean is defined as\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{1}{m} \\sum_{i=1}^{m} y_i\n",
    "$$\n",
    "\n",
    "Substituting this into the likelihood function, we get:\n",
    "\n",
    "$$\n",
    "p(\\vec{y};\\mu, \\sigma^2) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^m \\exp\\left(-\\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{m} y_i^2 - 2m\\mu \\bar{y} + m\\mu^2 \\right) \\right)\n",
    "$$\n",
    "\n",
    "We can factor this expression as:\n",
    "\n",
    "$$\n",
    "p(\\vec{y};\\mu, \\sigma^2) = \\exp\\left(-\\frac{m}{2\\sigma^2}(\\bar{y} - \\mu)^2\\right) \\cdot \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^m \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{m} y_i^2 + \\frac{m\\bar{y}^2}{\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Here, the likelihood function $p(\\vec{y}|\\mu)$ has been factored into two components:\n",
    "- $g(\\bar{y}, \\mu) = \\exp\\left(-\\frac{m}{2\\sigma^2}(\\bar{y} - \\mu)^2\\right)$, which depends on the data $\\vec{y}$, the sample mean $\\bar{y}$, and the parameter $\\mu$.\n",
    "- $h(\\vec{y}) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^m \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{m} y_i^2 + \\frac{m\\bar{y}^2}{\\sigma^2}\\right)$, which depends on the data $\\vec{y}$, the sample mean $\\bar{y}$, and the parameter $\\sigma^2$, but not on the parameter $\\mu$.\n",
    "\n",
    "According to the Factorization Theorem, this factorization confirms that the sample mean $\\bar{\\mathbf{y}}$ is a sufficient statistic for the parameter $\\mu$. \n",
    "\n",
    "The statistic $\\bar{\\mathbf{y}}$ captures all the information about $\\mu$ that is contained in the data $\\vec{\\mathbf{y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation of the likelihood function (joint PDF)\n",
    "\n",
    "Note that the likelihood function is the joint PDF of the observations $ \\mathbf{y}_1, \\mathbf{y}_2, \\ldots, \\mathbf{y}_m $.\n",
    "\n",
    "\n",
    "Specifically, given that $ \\mathbf{y}_1, \\mathbf{y}_2, \\ldots, \\mathbf{y}_m $ are independent and identically distributed (i.i.d.) random variables from a normal distribution $ \\mathcal{N}(\\mu, \\sigma^2) $, the joint pdf of these observations can be written as:\n",
    "\n",
    "$$\n",
    "f(\\vec{\\mathbf{y}};\\mu) = f(\\mathbf{y}_1, \\mathbf{y}_2, \\ldots, \\mathbf{y}_m; \\mu) = \\prod_{i=1}^{m} f(\\mathbf{y}_i ; \\mu)\n",
    "$$\n",
    "\n",
    "Since each $ \\mathbf{y}_i $ is normally distributed with mean $ \\mu $ and variance $ \\sigma^2 $, the pdf of each $ y_i $ is:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{y}_i ; \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Thus, the joint pdf is:\n",
    "\n",
    "$$\n",
    "f(\\vec{y} ; \\mu) = \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "This can be simplified to:\n",
    "\n",
    "$$\n",
    "f(\\vec{y} ; \\mu) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^m \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{m} (y_i - \\mu)^2\\right)\n",
    "$$\n",
    "\n",
    "This is the joint pdf of the observations $ \\mathbf{y}_1, \\mathbf{y}_2, \\ldots, \\mathbf{y}_m $, and it is also the likelihood function when considering $ \\mu $ as the parameter to be estimated from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: Sufficient Statistic $T(\\vec{\\mathbf{y}})$ vs. Estimator $\\hat{\\boldsymbol{\\alpha}}(\\vec{\\mathbf{y}})$\n",
    "\n",
    "- A sufficient statistic is a function of the data that captures all the information available in the data about a particular parameter.\n",
    "- An estimator is a rule or function that provides an estimate of a parameter based on the observed data. It is also typically a statistic (a function of the data) that is used to infer the value of an unknown parameter.\n",
    "\n",
    "Loosely speaking, a sufficient statistic is a statistic (a function of data), which can be used to serve as an estimator. A sufficient statistic often plays a dual role as a statistic and as an estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sufficient Statistic for The Sample Variance\n",
    "\n",
    "Consider independent and identically distributed (i.i.d.) Gaussian random variables, where each individual observation $\\mathbf{y}_i$ is normally distributed with mean $\\mu$ and variance $\\sigma^2$ as above.\n",
    "\n",
    "We know that the sample mean $\\bar{\\mathbf{y}}$ is a sufficient statistic for estimating the true mean $\\mu$. Now, what is a sufficient statistic for estimating the true variance $\\sigma^2$?\n",
    "\n",
    "We have that the sample variance $\\mathbf{s}_{\\mathbf{y}}^2$, where\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_{\\mathbf{y}}^2 = \\frac{1}{m} \\sum_{i=1}^{m}(\\mathbf{y}_i - \\bar{\\mathbf{y}})^2\n",
    "$$\n",
    "\n",
    "Note that the expression \n",
    "\n",
    "$$\n",
    "\\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{m} y_i^2 + \\frac{m\\bar{y}^2}{\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "we can rewrite it in terms of the sample variance $ s_y^2 = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\bar{y})^2 $ as\n",
    "\n",
    "$$\n",
    "\\exp\\left(-\\frac{m s_y^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "p(\\vec{y}|\\mu) = \\exp\\left(-\\frac{m}{2\\sigma^2}(\\bar{y} - \\mu)^2\\right) \\cdot \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^m \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{m} y_i^2 + \\frac{m\\bar{y}^2}{\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "can be expressed as\n",
    "\n",
    "$$\n",
    "p(\\vec{y}|\\mu) = \\exp\\left(-\\frac{m}{2\\sigma^2}(\\bar{y} - \\mu)^2\\right) \\cdot \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^m \\exp\\left(-\\frac{m s_y^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Therefore, the **sample variance $ s_y^2 $** is also a sufficient statistic for the variance $\\sigma^2$. \n",
    "\n",
    "Therefore, the **sample variance $ s_{\\mathbf{y}}^2 $** is also a sufficient statistic for the variance $\\sigma^2$. \n",
    "\n",
    "Note that\n",
    "- The **sample mean $\\bar{y}$** is a sufficient statistic for the mean $\\mu$ of the distribution. \n",
    "    - This means it contains all the necessary information about $\\mu$ present in the data. \n",
    "    - Moreover, $\\bar{y}$ is an unbiased estimator of the true mean $\\mu$, meaning that its expected value equals $\\mu$.\n",
    "\n",
    "- The **sample variance $ s_{\\mathbf{y}}^2 $**, on the other hand, is also a sufficient statistic for the variance $\\sigma^2$. \n",
    "    - It captures all the relevant information about the variance from the data. \n",
    "    - However, when used as an estimator of the true variance $\\sigma^2$, $ s_{\\mathbf{y}}^2 $ is a biased estimator. \n",
    "    - Specifically, the expectation of $ s_{\\mathbf{y}}^2 $ is $\\frac{m-1}{m}\\sigma^2$, which is slightly lower than the true variance $\\sigma^2$. \n",
    "    - To correct this bias, the unbiased estimator of the variance is $\\frac{m}{m-1} s_{\\mathbf{y}}^2$, often denoted as $ s^2_{\\text{unbiased}} $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the **sample variance $ s_{\\mathbf{y}}^2 $** is also a sufficient statistic for the variance $\\sigma^2$. \n",
    "\n",
    "Note that\n",
    "- The **sample mean $\\bar{y}$** is a sufficient statistic for the mean $\\mu$ of the distribution. \n",
    "    - This means it contains all the necessary information about $\\mu$ present in the data. \n",
    "    - Moreover, $\\bar{y}$ is an unbiased estimator of the true mean $\\mu$, meaning that its expected value equals $\\mu$.\n",
    "\n",
    "- The **sample variance $ s_{\\mathbf{y}}^2 $**, on the other hand, is also a sufficient statistic for the variance $\\sigma^2$. \n",
    "    - It captures all the relevant information about the variance from the data. \n",
    "    - However, when used as an estimator of the true variance $\\sigma^2$, $ s_{\\mathbf{y}}^2 $ is a biased estimator. \n",
    "    - Specifically, the expectation of $ s_{\\mathbf{y}}^2 $ is $\\frac{m-1}{m}\\sigma^2$, which is slightly lower than the true variance $\\sigma^2$. - To correct this bias, the unbiased estimator of the variance is $\\frac{m}{m-1} s_{\\mathbf{y}}^2$, often denoted as $ s^2_{\\text{unbiased}} $. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
